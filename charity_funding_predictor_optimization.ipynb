{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fe9b35",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e615ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for machine learning/neural network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "# for data handling\n",
    "import pandas as pd\n",
    "\n",
    "# general use\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ceb2b",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in the raw data\n",
    "df0 = pd.read_csv(join(\"resources\", \"charity_data.csv\"))\n",
    "\n",
    "# preview the raw data\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EIN and NAME are unnecessary for the neural net, so we'll drop them from the dataset\n",
    "df1 = df0.drop([\"EIN\", \"NAME\", \"STATUS\"], axis = 1)\n",
    "\n",
    "# preview the data\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc73a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define unique item threshold\n",
    "unique_item_count = 10\n",
    "\n",
    "# check for columns that require modification\n",
    "modify_columns = []\n",
    "value_count_lists = []\n",
    "for col in df1.nunique().items():\n",
    "    if (col[1] > unique_item_count) and (df1[col[0]].dtype == \"object\"):\n",
    "        modify_columns.append(col[0])\n",
    "        value_count_lists.append(df1[col[0]].value_counts())\n",
    "        print(df1[col[0]].value_counts())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify cutoff values for the relevant columns\n",
    "cutoff_values = [100, 10]\n",
    "\n",
    "# modify the specified columns\n",
    "for i in range(len(modify_columns)):\n",
    "    \n",
    "    # assemble a list of items to be replaced\n",
    "    items_to_replace = []\n",
    "    for item in value_count_lists[i].items():\n",
    "        if item[1] < cutoff_values[i]:\n",
    "            items_to_replace.append(item[0])\n",
    "    \n",
    "    # replace the items of the associated column\n",
    "    for item in items_to_replace:\n",
    "        df1[modify_columns[i]] = df1[modify_columns[i]].replace(item, \"other\")\n",
    "    \n",
    "    # display the modified column\n",
    "    print(df1[modify_columns[i]].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbaf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace categorical data with numerical data\n",
    "df2 = pd.get_dummies(df1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ffa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features and outputs\n",
    "y = df2[\"IS_SUCCESSFUL\"].values\n",
    "X = df2.drop(\"IS_SUCCESSFUL\", axis = 1).values\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)\n",
    "\n",
    "# create and fit the scaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# scale the features\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b647d",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the preprocessing steps\n",
    "def prepare_data(df, removed_columns, cutoff_values, unique_item_count):\n",
    "    df_1 = df.drop(removed_columns, axis = 1)\n",
    "    \n",
    "    # check for columns that require modification\n",
    "    modify_columns = []\n",
    "    value_count_lists = []\n",
    "    for col in df_1.nunique().items():\n",
    "        if (col[1] > unique_item_count) and (df_1[col[0]].dtype == \"object\"):\n",
    "            modify_columns.append(col[0])\n",
    "            value_count_lists.append(df_1[col[0]].value_counts())\n",
    "    \n",
    "    # modify the specified columns\n",
    "    for i in range(len(modify_columns)):\n",
    "\n",
    "        # assemble a list of items to be replaced\n",
    "        items_to_replace = []\n",
    "        for item in value_count_lists[i].items():\n",
    "            if item[1] < cutoff_values[i]:\n",
    "                items_to_replace.append(item[0])\n",
    "\n",
    "        # replace the items of the associated column\n",
    "        for item in items_to_replace:\n",
    "            df_1[modify_columns[i]] = df_1[modify_columns[i]].replace(item, \"other\")\n",
    "\n",
    "    # replace categorical data with numerical data\n",
    "    df_2 = pd.get_dummies(df_1)\n",
    "\n",
    "    # define features and outputs\n",
    "    y = df_2[\"IS_SUCCESSFUL\"].values\n",
    "    X = df_2.drop(\"IS_SUCCESSFUL\", axis = 1).values\n",
    "\n",
    "    # split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)\n",
    "\n",
    "    # create and fit the scaler\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    # scale the features\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return (X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c5b82",
   "metadata": {},
   "source": [
    "### Neural Net Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the neural net model for quick iterations\n",
    "def quick_build_model(X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "    \n",
    "    # initialize the neural net model\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # define the input layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units = 80, activation = \"relu\", input_dim = X_train_scaled.shape[1]))\n",
    "\n",
    "    # define the hidden layer(s)\n",
    "    nn_model.add(tf.keras.layers.Dense(units = 30, activation = \"relu\"))\n",
    "\n",
    "    # define the output layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "    # show the model's summary\n",
    "    nn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    # fit the model\n",
    "    nn_model.fit(X_train_scaled, y_train, epochs = 20, verbose = 0)\n",
    "    \n",
    "    # return the model's evaluation\n",
    "    return nn_model.evaluate(X_test_scaled, y_test, verbose = 0)\n",
    "\n",
    "# build the neural net model for keras tuner\n",
    "def tuner_build_model(hp):\n",
    "    \n",
    "    # instantiate the model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # populate activation function options\n",
    "    activation_options = hp.Choice(\"activation\", [\"relu\", \"tanh\", \"sigmoid\"])\n",
    "    \n",
    "    # populate initial layer neurons\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units = hp.Int(\"first_units\", \n",
    "                       min_value = 2, \n",
    "                       max_value = 6, \n",
    "                       step = 2), \n",
    "        activation = activation_options, \n",
    "        input_dim = X_train_scaled.shape[1]))\n",
    "    \n",
    "    # populate hidden layer neurons\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 6)):\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units = hp.Int(\"units_\" + str(i),\n",
    "                          min_value = 2,\n",
    "                          max_value = 6,\n",
    "                          step = 2),\n",
    "            activation = activation_options))\n",
    "    \n",
    "    # populate output layer neurons\n",
    "    model.add(tf.keras.layers.Dense(units = 1, activation = \"sigmoid\"))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    # return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d87a8",
   "metadata": {},
   "source": [
    "### Optimize with Iterative Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\n",
    "    [\"EIN\", \"NAME\", \"AFFILIATION\"],\n",
    "    [\"EIN\", \"NAME\", \"USE_CASE\"],\n",
    "    [\"EIN\", \"NAME\", \"ORGANIZATION\"],\n",
    "    [\"EIN\", \"NAME\", \"STATUS\"],\n",
    "    [\"EIN\", \"NAME\", \"INCOME_AMT\"],\n",
    "    [\"EIN\", \"NAME\", \"SPECIAL_CONSIDERATIONS\"],\n",
    "    [\"EIN\", \"NAME\", \"ASK_AMT\"]]\n",
    "\n",
    "cutoff_values_to_check = [\n",
    "    [10, 10],\n",
    "    [100, 10],\n",
    "    [500, 10],\n",
    "    [1000, 10],\n",
    "    [10, 500],\n",
    "    [100, 500],\n",
    "    [500, 500],\n",
    "    [1000, 500],\n",
    "    [10, 1000],\n",
    "    [100, 1000],\n",
    "    [500, 1000],\n",
    "    [1000, 1000],\n",
    "    [10, 2000],\n",
    "    [100, 2000],\n",
    "    [500, 2000],\n",
    "    [1000, 2000]]\n",
    "\n",
    "columns = []\n",
    "cv0 = []\n",
    "cv1 = []\n",
    "losses = []\n",
    "accuracies = []\n",
    "for removed_columns in columns_to_remove:\n",
    "    for cutoff_values in cutoff_values_to_check:\n",
    "        print(f\"Columns: {removed_columns}\")\n",
    "        print(f\"Values: {cutoff_values}\")\n",
    "        iX_train_scaled, iX_test_scaled, iy_train, iy_test = prepare_data(df0, removed_columns, cutoff_values, 10)\n",
    "        model_loss, model_acc = quick_build_model(iX_train_scaled, iX_test_scaled, iy_train, iy_test)\n",
    "        print(f\"Loss: {model_loss:,.4f}, Accuracy: {model_acc:,.4f}\")\n",
    "        columns.append(removed_columns[-1])\n",
    "        cv0.append(cutoff_values[0])\n",
    "        cv1.append(cutoff_values[1])\n",
    "        losses.append(model_loss)\n",
    "        accuracies.append(model_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"loss\": losses,\n",
    "    \"acc\": accuracies,\n",
    "    \"cv_0\": cv0,\n",
    "    \"cv_1\": cv1,\n",
    "    \"column\": columns\n",
    "})\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[results_df[\"loss\"] == results_df[\"loss\"].min()]\n",
    "# when loss is minimized...\n",
    "# loss    -> 0.549294\n",
    "# acc     -> 0.731195\n",
    "# cv_0    -> 10\n",
    "# cv_1    -> 10\n",
    "# column  -> SPECIAL_CONSIDERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe10288",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[results_df[\"acc\"] == results_df[\"acc\"].max()]\n",
    "# when accuracy is maximized\n",
    "# loss    -> 0.550659\n",
    "# acc     -> 0.733294\n",
    "# cv_0    -> 100\n",
    "# cv_1    -> 10\n",
    "# column  -> STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9337a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(losses, accuracies)\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"acc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ce231",
   "metadata": {},
   "source": [
    "If we remove the `STATUS` column and use cutoff values of 100 and 10 for `APPLICATION_TYPE` and `CLASSIFICATION` respectively, then we attain a little more accuracy and less loss than the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88233865",
   "metadata": {},
   "source": [
    "### Compile, Train, and Evaluate with Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    tuner_build_model,\n",
    "    objective = \"val_accuracy\",\n",
    "    max_epochs = 20,\n",
    "    overwrite = True,\n",
    "    hyperband_iterations = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the tuner\n",
    "tuner.search(X_train_scaled, y_train, epochs = 20, validation_data = (X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the highest performing hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# preview the hyperparameters\n",
    "best_hps.values\n",
    "\n",
    "# activation -> sigmoid\n",
    "# layer count -> 2\n",
    "# neuron count for layer 1 -> 6\n",
    "# neuron count for layer 2 -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b283c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model's performance\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "model_loss, model_acc = best_model.evaluate(X_test_scaled, y_test, verbose = 2)\n",
    "print(f\"Loss: {model_loss:,.4f}, Accuracy: {model_acc:,.4f}\")\n",
    "\n",
    "# OUTPUT...\n",
    "# 268/268 - 0s - loss: 0.5727 - accuracy: 0.7341 - 375ms/epoch - 1ms/step\n",
    "# Loss: 0.5702, Accuracy: 0.7350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "best_model.save(join(\"output\", \"alphabet_soup_charity_optimized.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601eddc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
